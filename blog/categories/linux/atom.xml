<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Linux | Wood Carver]]></title>
  <link href="http://woodcarver.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://woodcarver.github.io/"/>
  <updated>2017-01-15T01:00:53+08:00</updated>
  <id>http://woodcarver.github.io/</id>
  <author>
    <name><![CDATA[Wood Carver]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linux系统状态命令详解]]></title>
    <link href="http://woodcarver.github.io/blog/2016/02/17/deep-insight-of-linux-system-status-description-commands/"/>
    <updated>2016-02-17T21:58:48+08:00</updated>
    <id>http://woodcarver.github.io/blog/2016/02/17/deep-insight-of-linux-system-status-description-commands</id>
    <content type="html"><![CDATA[<h1>知识地图</h1>

<p>这篇文章整理的是对关于诊断linux系统状况的相关指标和命令。涉及到：</p>

<ol>
<li>I/O负载状况</li>
<li>进程控制</li>
<li>cpu负载状况</li>
<li>memory占用情况</li>
<li>网络状况</li>
<li>disk使用情况</li>
</ol>


<p>其中统计值会把I/O的时候见包含进cpu时间中，所以从上面的分类来看一个运行的进程的指标——cpu就是衡量任务的占时状况，而内存就是占用空间情况。</p>

<h1>Commands about I/O</h1>

<ul>
<li>什么是I/O</li>
</ul>


<p>再让我们看一眼csapp的那幅经典图
<img src="http://woodcarver.oss-cn-shanghai.aliyuncs.com/post/computer_organization.png" alt="computer organization" />
看到所有的设备都是通过I/O bridge把数据移动到Main memory上，然后再主存再和CPU交互进行计算。当CPU有输出时，也是先输出到主存上，然后再输入到设备上。而再计算机中I/O设备就只除去CPU和主存的设备，也成外围设备。
介绍完I/O设备，那什么是I/O呢？按照《Unix系统内幕》的定义：</p>

<blockquote><p>The I/O subsystem haldes the movement of data between memory and peripheral devices such as disks, printers, and terminals.
所以I/O就是指I/O设备和memory之间数据移动。</p></blockquote>

<p>怎么才算是一次I/O，大概是指移动次数但是这个不重要，重要的是对于I/O的衡量却不是根据其移动次数，而是根据其占用一个运行任务的时间比例衡量的。所以如果运行的进程中你看到真正使用CPU时间的比例很少，“大部分”是在等待I/O。那么就是说I/O不行，跟不上CPU的节奏。</p>

<p>那当发现I/O占时过大的时候怎么办？目标就是减少I/O！这个目标说起来容易，做起来也确实有几个常规方法。一种是数据库系统经常用到了。例如mysql的索引技术就是排除不相关数据，只传当前相关的数据。还有些对磁盘的数据进行压缩，这个传输到内存上的容量就少了，I/O自然降了。
还有就是换设备，换硬盘、换网卡<code>^_^</code>。</p>

<h2>iostat</h2>

<h2>top</h2>

<p>top对I/O的描述是用I/O wait这个指标，例如下图：
<img src="http://woodcarver.oss-cn-shanghai.aliyuncs.com/post/top_command_short.png" alt="top command" />
其中I/O wait 的定义是：</p>

<blockquote><p>I/O wait is the percentage of time your processors are waiting on the disk.
I/O wait就是CPU在整个任务处理中等待闲置的时间，举个例子：一个任务一共使用了1s，但是其中从从磁盘中取数据花了700ms，而在磁盘读取数据时CPU是闲置的，所以CPU的等待I/O的时间占了70%，即I/O wait 是70%。
当然有人指出对多核cpu通过iowait来统计io的负载情况并不准确，<a href="http://veithen.github.io/2013/11/18/iowait-linux.html">详见</a></p></blockquote>

<p><code>threshold：当I/O wait &gt; 1/CPU_cores 可以判断明显出现了I/O瓶颈(why?)。</code></p>

<h2>sar</h2>

<p>同样使用I/O wait这个指标，每10mini(默认时间间隔，可以调节)纪录一次系统状态值,保留一段时间的历史数据。</p>

<h2>dstat</h2>

<p>发现这个命令组合很有意思
dstat -tdD total,sda,sdb,sdc,md1 60</p>

<h1>command about CPU</h1>

<h2>top</h2>

<p>top对cpu的描述有下面几个值：
user &ndash; 表示用户太消耗的cpu时间
sys &ndash; 表示内核态消耗的cpu时间
real &ndash; 表示操作从开始到结束所经过的墙钟时间（Wall Clock Time）
cpu时间和墙钟时间的区别是，墙钟时间包括各种非运算的等待时间，例如等待磁盘I/O，等待线程阻塞，而cpu时间不包括这些耗时。但是当系统有多个cpu或者多核的时候，多线程操作会叠加这些cpu时间，所以如果看到user+sys > real 是正常的。</p>

<h2>Reference</h2>

<ul>
<li>[深入理解计算机系统（第二版）]</li>
<li>[Unix系统内幕]</li>
<li><a href="http://blog.scoutapp.com/articles/2011/02/10/understanding-disk-i-o-when-should-you-be-worried">understanding disk i/o - when should you be worried?</a></li>
<li><a href="http://veithen.github.io/2013/11/18/iowait-linux.html">The precise meaning of I/O wait time in Linux</a></li>
</ul>


<h1>进程控制</h1>

<ul>
<li>exec</li>
<li>source</li>
<li>kill</li>
<li>bg,fg,jobs</li>
<li>nice</li>
</ul>


<h1>网络状况</h1>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[网络编程中的服务器模型]]></title>
    <link href="http://woodcarver.github.io/blog/2016/01/05/networking-server/"/>
    <updated>2016-01-05T22:14:11+08:00</updated>
    <id>http://woodcarver.github.io/blog/2016/01/05/networking-server</id>
    <content type="html"><![CDATA[<p>网络编程中的服务器模型根据接受请求和处理请求的方法来分，大方向分为阻塞模型和非阻塞模型。而根据服务器对请求派生的进程或者线程方式，模型又分为单进程或线程和多进程或线程。下面依次讲解这几种经典模型：</p>

<h2>单进程的阻塞模型</h2>

<p>这是服务器模型中最为简单的模型，当然处理客户端请有的效率也是最低的。该模型是启动一个socket进程，每来一个请求就处理一个，如果同时有多个请求，只处理最先到达的请求，其他的请求处于阻塞状态直到前一个请求处理完成。尽管这个模型很简单，但是理解其他网络服务器模型的基石。其参考代码如下(代码下载：)：</p>

<p>下面的例子的功能是把服务端的时间传送给客户端：</p>

<pre><code>服务端程序：
</code></pre>

<pre><code class="c">    int main(int argc,char **argv)
    {
        int sockfd,bin,connfd,lisfd;
        char buff[MAXLINE];
        struct sockaddr_in sevr_addr;
        time_t ticks;

        memset(&amp;sevr_addr,0,sizeof(sevr_addr));
        sevr_addr.sin_family=AF_INET;
        sevr_addr.sin_addr.s_addr=htonl(INADDR_ANY);
        sevr_addr.sin_port=htons(SERVER_PORT);

        sockfd=socket(AF_INET,SOCK_STREAM,IPPROTO_TCP);
        if(sockfd&lt;0)
            fatal("socket failed");
        bin=bind(sockfd,(struct sockaddr *)&amp;sevr_addr,sizeof(sevr_addr));
        if(bin&lt;0)
            fatal("bind failed");

        lisfd=listen(sockfd,QUEUE_SIZE);
        if(lisfd&lt;0)
            fatal("listen failed");

        while(1){
            connfd=accept(sockfd,NULL,NULL);
            ticks=time(NULL);
            printf("send time:%d\n",ticks);
            snprintf(buff,sizeof(buff),"%.24s\r\n",ctime(&amp;ticks));
            write(connfd,buff,strlen(buff));
            close(connfd);
        }
        exit(0);
    }
</code></pre>

<pre><code>该例子中，只启用了一个accept来接受客户端的请求连接，每次只处理一个请求。其他的请求则按照FIFO的原则排入队列中，队列的大小是作为listen函数的第二个参数通知系统的。关于socket编程，参考《unix网络编程》。

客户端程序：
</code></pre>

<pre><code class="c">    int main(int argc,char **argv)
    {
            int sockfd,n;
            char buff[MAXLINE+1];
            struct sockaddr_in sevr_addr;

            if((sockfd=socket(AF_INET,SOCK_STREAM,0))&lt;0)
                    fatal("socket error");
            memset(&amp;sevr_addr,0,sizeof(sevr_addr));
            sevr_addr.sin_family=AF_INET;
            sevr_addr.sin_port=htons(SERVER_PORT);
            if(inet_pton(AF_INET,argv[1],&amp;sevr_addr.sin_addr)&lt;=0)
                    fatal("can not reach the address");
            if(connect(sockfd,(struct sockaddr *)&amp;sevr_addr,sizeof(sevr_addr))&lt;0)
                    fatal("can not connect");
            while((n=read(sockfd,buff,MAXLINE))&gt;0){
                    buff[n]=0;
                    if(fputs(buff,stdout)==EOF)
                            fatal("fputs error");
            }
            if(n&lt;0)
                    fatal("read error");
            exit(0);
    }
</code></pre>

<pre><code>下面的模型，变化的只是服务端程序，客户端的程序是不变得。
</code></pre>

<h2>一个进程或者线程一个请求模型</h2>

<p>这种模型是在单进程模型的基础上，对每个请求派生一个专属的进程。由于是多进程或者线程，这种模型是可以同时处理多个请求。但是这种模型存在派生进程过多，导致服务器崩溃的危险情况。同时由于大量的进程或者线程的生成和销毁，会耗费大量的cpu资源。而其他高级模型就是解决这种情况而演变出来的。其编程的核心是在单进程模型上利用fork函数。</p>

<pre><code>服务端代码示例：
</code></pre>

<pre><code>    #include&lt;stdio.h&gt;
    #include&lt;stdlib.h&gt;
    #include&lt;string.h&gt;
    #include&lt;netinet/in.h&gt;

    #define MAXLINE 100
    #define SERVER_PORT 12345
    #define QUEUE_SIZE 1

    void fatal(char *msg)
    {
        printf("%s\n",msg);
        exit(1);
    }
    int main(int argc,char **argv)
    {
        int sockfd,bin,connfd,lisfd;
        char buff[MAXLINE];
        struct sockaddr_in sevr_addr;
        time_t ticks;
        pid_t pid;

        memset(&amp;sevr_addr,0,sizeof(sevr_addr));
        sevr_addr.sin_family=AF_INET;
        sevr_addr.sin_addr.s_addr=htonl(INADDR_ANY);
        sevr_addr.sin_port=htons(SERVER_PORT);

        sockfd=socket(AF_INET,SOCK_STREAM,IPPROTO_TCP);
        if(sockfd&lt;0)
            fatal("socket failed");
        bin=bind(sockfd,(struct sockaddr *)&amp;sevr_addr,sizeof(sevr_addr));
        if(bin&lt;0)
            fatal("bind failed");

        lisfd=listen(sockfd,QUEUE_SIZE);
        if(lisfd&lt;0)
            fatal("listen failed");

        while(1){
            connfd=accept(sockfd,NULL,NULL);
            if((pid=fork())==0){
                close(lisfd);/*why?*/

                ticks=time(NULL);
                printf("send time:%d\n",ticks);
                snprintf(buff,sizeof(buff),"%.24s\r\n",ctime(&amp;ticks));
                write(connfd,buff,strlen(buff));

                printf("sleep for 10 seconds...\n");
                sleep(10);
                printf("the connect is over.\n");

                close(connfd);
                exit(0);
            }
            close(connfd);
        }
    }
</code></pre>
]]></content>
  </entry>
  
</feed>
